id,name,type,content
0,Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning,多模态预训练模型,采用了一个下双流+上单流的架构，同时单模态Transformer会延展到上层，是一个12+12+6的总架构。上层的单模态层会与交叉层做额外的交互，因此结果会相当的好
1,Multimodal Contrastive Learning with LIMoE:the Language-Image Mixture of Experts,CLIP变种,一个共同的模型来做CLIP的双塔，利用MOE来分别进行图像和文本的编码
2,How Much Can CLIP Benefit Vision-and-Language Tasks?,CLIP变种,通过CLIP进行一系列的下游任务
3,SimCSE: Simple Contrastive Learning of Sentence Embeddings,句子嵌入,通过单一的encoder做对比学习，无监督通过不同的dropout例子，有监督关系数据集来做正负例
4,MCSE: Multimodal Contrastive Learning of Sentence Embeddings,视觉增强文本表示学习,利用图像来增强simcse，利用文本的相应图片来做正负例，并在STS数据集上取得了不错的结果
5,Cross-Modal Discrete Representation Learning,多模态表示学习,用离散的embedding去尝试表示不同模态的数据，增加了CMCL损失函数来保证相似的输入的codebook分布是相近的，最终不仅提高了性能，同时也通过离散的token得到了对实验结果的可解释性
6,MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices,预训练模型压缩,主体方法为知识蒸馏，但是采用了一个同样深与BERT-LARGE但会窄很多的老师模型为一个特殊的BERT-LARGE模型，中间采用类似adapter的结构来减少中间隐藏层参数量。同时提出了一系列训练与结构trick来辅助知识蒸馏进程
7,"DistilBERT, a distilled version of BERT: smaller,faster, cheaper and lighter",预训练模型压缩,单纯的知识蒸馏，去掉了token type embedding与pooler，把层数压缩到只有一半
8,TinyBERT: Distilling BERT for Natural Language Understanding,预训练模型压缩,采用一个两阶段的知识蒸馏，先对预训练模型进行知识蒸馏，再在微调时进行task-specific的知识蒸馏。同时采取了word-replacement的知识增强。最终蒸馏得到了一个四层的Bert
9,ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS,预训练模型压缩,采用了层参数共享，embedding截断以及增加数据量等方式压缩albert，使其参数量缩减至10%，但是保持了结果。问题在于并没有加快推理速度
10,Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators ,预训练模型轻量化微调,用MPO进行张量分解，对核心张量冻结，微调辅助张量
11,Exploiting Redundancy in Pre-trained Language Models for Efficient Transfer Learning,预训练模型信息冗余,预训练模型中存在着信息冗余，其表现为：浅层的特征也可以做很好的下游任务特征，即上层的结构冗余。或者大量特征趋向于同质性，相似度冗余。还有抽取出的各类特征不一定均会对下游任务起作用，即特征冗余。采取了两种方式分别裁剪层数与特征数，削减了冗余，并取得了不错的下游任务效果
12,Analyzing Redundancy in Pretrained Transformer Models,预训练模型信息冗余,与上一篇基本相同，但是添加了冗余性的定义
13,Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing,预训练模型冗余,在encoder-decoder的encode过程中，通过池化来减少输入序列长度，在decode过程中还原序列长度以减少flops
14,"Drop Redundant, Shrink Irrelevant: Selective Knowledge Injection for Language Pretraining",预训练模型冗余在向预训练模型注,在向预训练模型注入知识的时候，可能存在一些知识已经从语料中获取过，会存在冗余或不相关的知识，影响预训练模型本身的性能，需要进行排除
15,A Unified Sequence Interface for Vision Tasks,unified多模态模型,将四个视觉任务统一到一seq2seq任务中，并利用prompt引导任务相关的输出
16,Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations,CLI多模态embedding与PLM embedding分析,对CLIP和GPT2的word embedding与sentence embedding做了相似度计算与分析
17,DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings,预训练表征学习,通过一种基于diff的方式，采用一个预训练好的PLM，通过MLM来生成增强样本，待训练的PLM会对文本进行encoding，生成的embedding作为单独的向量来对增强样本进行引导，最后训练一个分类层来观察两个样本之间的difference，从而得到更好的预训练表示
18,A Unified Sequence Interface for Vision Tasks,Unified VLP,通过一系列的文本prompt多模态模型进行创传统视觉任务
19,NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better,预训练模型微调,在微调前直接对参数矩阵加入一定噪声，帮助缓解过拟合
20,MiniVLM: A Smaller and Faster Vision-Language Model,预训练模型压缩,采用了一个轻量级的目标检测器与fusion transformer进行压缩，得到一个轻量级的VLP
21,Compressing Visual-linguistic Model via Knowledge Distillation,预训练模型压缩,此前工作的延申，通过知识蒸馏手段压缩各个部分的层数以及大小
22,MobiVQA: Efficient On-Device Visual Question Answering,预训练模型压缩,通过early-exit手段提前退出，通过pruning手段剪枝掉不需要的像素，舍弃掉了目标检测器，最终得到了一个移动端运行的快速VQA
23,MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers,预训练模型压缩,改进版的对Bert的知识蒸馏
24,Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks,Unified VL,BeiT v3:采用VLMo作为backbone，将所有不同的视觉，语言，v-l预训练任务统一到一个masked data modeling中，在公开数据集上训练，相对于所有的模型都取得了很大成功。
25,VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts,Unified VL,采用了混合专家的FFN来分别处理视觉，文本，视觉-文本输入，共享attention层。VLMo可以同时进行CLIP-style image-text contrastive learning与fusion-style MLM，在下游任务上取得很大成功
26,BEIT: BERT Pre-Training of Image Transformers,image pre-training,首先将图像表示成patches，每个patch通过dVAE映射到一个discrete token，然后再decode复原（dVAE过程）。预训练时对图像进行随机mask，mask后的图像输入的image transformer，并要求predict出对应的visual token
27,How Much Can CLIP Benefit Vision-and-Language Tasks?,CLIP-related,采用CLIP作为核心去进行各类的下游任务
28,IDEA: Increasing Text Diversity via Online Multi-Label Recognition for Vision-Language Pre-training,CLIP变种,通过多标签识别，对图-文对语料中存在不清晰指代的文本描述进行指代替换（替换为识别出的tag），生成更加多样化，更加详细的文本描述，并提高视觉编码器的能力
29,LANGUAGE MODELLING WITH PIXELS,奇怪的语言模型,将所有语言用文字表示出来，将文本任务视作视觉识别任务来做预训练
30,Learning More May Not Be Better: Knowledge Transferability in Vision and Language Tasks,V-L模型任务迁移研究,对12个V-L下游任务分组采用两次微调的方式研究source task到target task的积极或消极作用，并研究了可能的原因
31,Bi-VLDoc: Bidirectional Vision-Language Modeling for Visually-Rich Document Understanding,多模态文档理解,LayoutLM的一个后续工作
32,Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation,CLIP蒸馏应用,为了解决CLIP模型无法做生成任务的问题，采用了一个BART，通过知识蒸馏的方式与CLIP的image encoder对齐，并通过三个训练函数来训练BART的V-L对齐能力，V-enhanced生成能力以及与CLIP textencoder的对齐能力，并在一系列caption任务上取得了不错的效果
33,OPEN-VOCABULARY OBJECT DETECTION VIA VISION AND LANGUAGE KNOWLEDGE DISTILLATION,CLIP蒸馏应用,"为了解决在目标检测时，目标检测类别有限的问题，通过大量开放领域图-文对训练的CLIP进行知识蒸馏。目标检测的RP通过裁剪到同一尺寸后送入CLIP的image encoder,对应类别通过prompt生成语句后送入text encoder，最终进行一个alignment。传统的类别classifier也被生成的text embedding所替代。"
34,UniMS: A Unified Framework for Multimodal Summarization with Knowledge Distillation,CLIP蒸馏应用,在做多模态摘要时，首先需要进行一个图像选择，再进行摘要生成，由于CLIP本身包含了400M图-文对的信息，可以通过将CLIP知识蒸馏到BART encoder的图像端来辅助其完成一个图像检索工作，检索完成后，通过两个特征的拼接再送入一个decoder来生成摘要。
35,LiT : Zero-Shot Transfer with Locked-image text Tuning,CLIP实验,google通过一系列的实验，包括设置双塔是否冻结，是否加载预训练权重或随机初始化等等，试出了最佳的预训练方案是Lock image and random initialize text
